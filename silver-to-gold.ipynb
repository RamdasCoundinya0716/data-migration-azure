{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e5f5e9-16d9-42c1-b67a-0745c36a2aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_all_delta_tables(base_path):\n",
    "    # Get all subfolders in the SalesLT directory\n",
    "    folders = [f.name for f in dbutils.fs.ls(base_path) if f.isDir() and f.name != '_delta_log']\n",
    "    \n",
    "    dataframes = {}\n",
    "    \n",
    "    for folder in folders:\n",
    "        folder_path = f\"{base_path}/{folder}\"\n",
    "        \n",
    "        # List the files in the folder\n",
    "        files = dbutils.fs.ls(folder_path)\n",
    "        \n",
    "        # Check if the folder contains any parquet files\n",
    "        parquet_files = [file for file in files if file.name.endswith(\".parquet\")]\n",
    "        \n",
    "        if parquet_files:\n",
    "            try:\n",
    "                # Read the Delta table using the folder path\n",
    "                df = spark.read.format(\"delta\").load(folder_path)\n",
    "                \n",
    "                # Extract folder name without any slashes\n",
    "                folder_name = os.path.basename(folder.rstrip('/'))\n",
    "                \n",
    "                # Create the DataFrame name as per the naming convention\n",
    "                dataframe_name = f\"{folder_name[:1].lower() + folder_name[1:]}_modified\"\n",
    "                \n",
    "                # Store the DataFrame in globals() to create a variable with the desired name\n",
    "                globals()[dataframe_name] = df\n",
    "                dataframes[dataframe_name] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {folder}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Define the base path to SalesLT\n",
    "base_path = \"/mnt/silver/SalesLT\"\n",
    "\n",
    "# Read all Delta tables from the SalesLT folder\n",
    "dataframes = read_all_delta_tables(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4106b301-d63d-4861-b331-51feffdbf6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def standardize_column_names(dataframe: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes column names by converting them from camel case to capitalized words separated by underscores.\n",
    "    Example: ThisIsColumn -> This_Is_Column\n",
    "\n",
    "    Args:\n",
    "    dataframe (DataFrame): The input DataFrame with original column names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with standardized column names.\n",
    "    \"\"\"\n",
    "    def split_camel_case(column_name: str) -> str:\n",
    "        result = []\n",
    "        for i, char in enumerate(column_name):\n",
    "            if i > 0 and char.isupper() and column_name[i - 1].islower():\n",
    "                result.append('_')\n",
    "            result.append(char)\n",
    "        \n",
    "        # Convert the list of characters back to a string\n",
    "        split_name = ''.join(result)\n",
    "        \n",
    "        # Capitalize words and join with '_'\n",
    "        final_name = '_'.join([word.capitalize() for word in split_name.split('_')])\n",
    "        return final_name\n",
    "    \n",
    "    # Store modified column names\n",
    "    columns = dataframe.columns\n",
    "    renamed_columns = [split_camel_case(col) for col in columns]\n",
    "    \n",
    "    # Rename columns in the DataFrame using selectExpr\n",
    "    renamed_exprs = [f\"`{col}` as `{new_col}`\" for col, new_col in zip(columns, renamed_columns)]\n",
    "    standardized_df = dataframe.selectExpr(*renamed_exprs)\n",
    "    \n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58522b56-a48d-4778-9973-d59ec1358c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_standardization_and_overwrite(dataframes: dict) -> None:\n",
    "    \"\"\"\n",
    "    Applies column standardization to all DataFrames and rewrites the original ones.\n",
    "\n",
    "    Args:\n",
    "    dataframes (dict): Dictionary of DataFrames with keys as DataFrame names.\n",
    "    \"\"\"\n",
    "    for name, df in dataframes.items():\n",
    "        standardized_df = standardize_column_names(df)\n",
    "        \n",
    "        # Overwrite the original DataFrame in the dictionary and globals()\n",
    "        dataframes[name] = standardized_df\n",
    "        globals()[name] = standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebadf1ee-ef05-462c-9379-3e9695b7424f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all DataFrames\n",
    "dataframes = read_all_delta_tables(base_path)\n",
    "\n",
    "# Apply renaming to all DataFrames and update them in place\n",
    "apply_standardization_and_overwrite(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7154d70d-dc5b-49a9-9c23-7b8aea7f532a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved DataFrame 'address_modified' to '/mnt/gold/SalesLT/Address/'\n✅ Successfully saved DataFrame 'customer_modified' to '/mnt/gold/SalesLT/Customer/'\n✅ Successfully saved DataFrame 'customeraddress_modified' to '/mnt/gold/SalesLT/Customeraddress/'\n✅ Successfully saved DataFrame 'product_modified' to '/mnt/gold/SalesLT/Product/'\n✅ Successfully saved DataFrame 'productcategory_modified' to '/mnt/gold/SalesLT/Productcategory/'\n✅ Successfully saved DataFrame 'productdescription_modified' to '/mnt/gold/SalesLT/Productdescription/'\n✅ Successfully saved DataFrame 'productmodel_modified' to '/mnt/gold/SalesLT/Productmodel/'\n✅ Successfully saved DataFrame 'productmodelproductdescription_modified' to '/mnt/gold/SalesLT/Productmodelproductdescription/'\n✅ Successfully saved DataFrame 'salesorderdetail_modified' to '/mnt/gold/SalesLT/Salesorderdetail/'\n✅ Successfully saved DataFrame 'salesorderheader_modified' to '/mnt/gold/SalesLT/Salesorderheader/'\n"
     ]
    }
   ],
   "source": [
    "def process_all_dataframes(created_dataframes, base_folder=\"SalesLT\", layer=\"gold\"):\n",
    "    \"\"\"\n",
    "    Processes all created DataFrames by renaming columns and saving them to the specified layer.\n",
    "\n",
    "    Args:\n",
    "    created_dataframes (dict): Dictionary of DataFrame names to be processed.\n",
    "    base_folder (str): Base folder name for saving DataFrames (default: \"SalesLT\").\n",
    "    layer (str): The target layer to save DataFrames ('gold' by default).\n",
    "    \"\"\"\n",
    "    for dataframe_name in created_dataframes:\n",
    "        original_df = globals()[dataframe_name]\n",
    "        \n",
    "        # Apply column standardization\n",
    "        modified_df = standardize_column_names(original_df)\n",
    "        \n",
    "        # Update the global variable with the modified DataFrame\n",
    "        globals()[dataframe_name.lower()] = modified_df\n",
    "        \n",
    "        # Extract folder name from DataFrame name\n",
    "        folder_name = dataframe_name.capitalize().split('_')[0]\n",
    "        \n",
    "        # Define the path for saving the modified DataFrame in the gold layer\n",
    "        gold_path = f\"/mnt/{layer}/{base_folder}/{folder_name}/\"\n",
    "        \n",
    "        # Save DataFrame as Delta format to 'gold' container in overwrite mode\n",
    "        modified_df.write.format(\"delta\").mode(\"overwrite\").save(gold_path)\n",
    "\n",
    "# Example usage with your created_dataframes dictionary\n",
    "process_all_dataframes(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07cacee-2b04-4a6a-b65d-01d842355fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver-to-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}