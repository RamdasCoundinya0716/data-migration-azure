{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e336d8-ee33-43b5-b2e9-819adbe5f2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mounting \"Bronze container\" from ADLS Gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6955abf-9c86-45f2-959b-fd662d3362f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze container mounted successfully! at /mnt/bronze\n"
     ]
    }
   ],
   "source": [
    "storage_account_name = \"storagelayercloud\"\n",
    "container_name = \"bronze\"\n",
    "key_vault_name = \"data-migration-vault\"\n",
    "secret_name = \"storageaccntsecret\"\n",
    "\n",
    "# Retrieve the storage account key from Azure Key Vault\n",
    "scope = f\"kv-{key_vault_name}\"\n",
    "key = dbutils.secrets.get(scope=scope, key=secret_name)\n",
    "\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# Unmount if the mount point already exists\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mounting the bronze container\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": key}\n",
    "    )\n",
    "    \n",
    "    # Check if mount is successful\n",
    "    if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "        print(f\"{container_name} container mounted successfully! at {mount_point}\")\n",
    "    else:\n",
    "        print(False)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(False)\n",
    "    print(f\"Mounting failed due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "962e1fb5-a9a6-45e5-8189-0aef3927b482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mounting \"Silver container\" from ADLS Gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684417be-1a74-4913-877f-80889faf3926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver container mounted successfully! at /mnt/silver\n"
     ]
    }
   ],
   "source": [
    "storage_account_name = \"storagelayercloud\"\n",
    "container_name = \"silver\"\n",
    "key_vault_name = \"data-migration-vault\"\n",
    "secret_name = \"storageaccntsecret\"\n",
    "\n",
    "# Retrieve the storage account key from Azure Key Vault\n",
    "scope = f\"kv-{key_vault_name}\"\n",
    "key = dbutils.secrets.get(scope=scope, key=secret_name)\n",
    "\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# Unmount if the mount point already exists\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mounting the bronze container\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": key}\n",
    "    )\n",
    "    \n",
    "    # Check if mount is successful\n",
    "    if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "        print(f\"{container_name} container mounted successfully! at {mount_point}\")\n",
    "    else:\n",
    "        print(False)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(False)\n",
    "    print(f\"Mounting failed due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b38b112-0ddf-4437-bcf7-38c870f31c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mounting \"Gold container\" from ADLS Gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d6571d-e867-4c78-a285-989f3d00f69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold container mounted successfully! at /mnt/gold\n"
     ]
    }
   ],
   "source": [
    "storage_account_name = \"storagelayercloud\"\n",
    "container_name = \"gold\"\n",
    "key_vault_name = \"data-migration-vault\"\n",
    "secret_name = \"storageaccntsecret\"\n",
    "\n",
    "# Retrieve the storage account key from Azure Key Vault\n",
    "scope = f\"kv-{key_vault_name}\"\n",
    "key = dbutils.secrets.get(scope=scope, key=secret_name)\n",
    "\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# Unmount if the mount point already exists\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "# Mounting the bronze container\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs={f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": key}\n",
    "    )\n",
    "    \n",
    "    # Check if mount is successful\n",
    "    if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "        print(f\"{container_name} container mounted successfully! at {mount_point}\")\n",
    "    else:\n",
    "        print(False)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(False)\n",
    "    print(f\"Mounting failed due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841a384a-060c-419c-bcab-49eb181a46d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### listing all the files present in the bronze location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18a3bd5-f9da-4bc9-8931-2c2ac456798a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/mnt/bronze/SalesLT/Address/Address.parquet\ndbfs:/mnt/bronze/SalesLT/Customer/Customer.parquet\ndbfs:/mnt/bronze/SalesLT/CustomerAddress/CustomerAddress.parquet\ndbfs:/mnt/bronze/SalesLT/Product/Product.parquet\ndbfs:/mnt/bronze/SalesLT/ProductCategory/ProductCategory.parquet\ndbfs:/mnt/bronze/SalesLT/ProductDescription/ProductDescription.parquet\ndbfs:/mnt/bronze/SalesLT/ProductModel/ProductModel.parquet\ndbfs:/mnt/bronze/SalesLT/ProductModelProductDescription/ProductModelProductDescription.parquet\ndbfs:/mnt/bronze/SalesLT/SalesOrderDetail/SalesOrderDetail.parquet\ndbfs:/mnt/bronze/SalesLT/SalesOrderHeader/SalesOrderHeader.parquet\n"
     ]
    }
   ],
   "source": [
    "def list_all_files(path):\n",
    "    files = []\n",
    "    \n",
    "    try:\n",
    "        # List all files and folders in the current directory\n",
    "        items = dbutils.fs.ls(path)\n",
    "        \n",
    "        for item in items:\n",
    "            if item.isDir():\n",
    "                # If the item is a directory, recurse into it\n",
    "                files.extend(list_all_files(item.path))\n",
    "            else:\n",
    "                # If the item is a file, add it to the list\n",
    "                files.append(item.path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while accessing {path}: {e}\")\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5adb4df-8a40-4fe9-bce1-3ec2394f84a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dynamic DataFrame Creation\n",
    "\n",
    "For each file:\n",
    "-   Load it as a Spark DataFrame.\n",
    "-   Extract the file name (without extension) to create a DataFrame name.\n",
    "-   Store the DataFrame as a variable using globals().\n",
    "\n",
    "Tracking & Displaying:\n",
    "-   Keep track of created DataFrames in a list.\n",
    "-   Display all successfully created DataFrames.\n",
    "-   Accessing DataFrames:\n",
    "-   DataFrames are accessible directly by their dynamically generated names (e.g., customers_dataframe, orders_dataframe).\n",
    "\n",
    "Code Flow\n",
    "-   Read Files: read_files_dynamically()\n",
    "-   Access DataFrames: Directly by name (e.g., customers_dataframe.show()).\n",
    "-   List Created DataFrames: Printed at the end of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da2bf13-1f5e-4382-ba53-a5d1fc764461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_files_dynamically(base_path):\n",
    "    all_files = list_all_files(base_path)\n",
    "    created_dataframes = []  # To keep track of created DataFrame names\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            if file_path.endswith(\".parquet\"):\n",
    "                df = spark.read.format('parquet') \\\n",
    "                        .option('header', 'true') \\\n",
    "                        .option('inferSchema', 'true') \\\n",
    "                        .load(file_path)\n",
    "                \n",
    "                # Extract the file name without extension and directory path\n",
    "                file_name = os.path.basename(file_path).replace(\".parquet\", \"\")\n",
    "                \n",
    "                # Create a DataFrame name based on the file name\n",
    "                dataframe_name = f\"{file_name}_dataframe\"\n",
    "                \n",
    "                # Store the DataFrame as a variable with the desired name using globals()\n",
    "                globals()[dataframe_name.lower()] = df\n",
    "                \n",
    "                # Track the DataFrame name\n",
    "                created_dataframes.append(dataframe_name.lower())\n",
    "            else:\n",
    "                print(f\"Unsupported file format for: {file_path}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "    return created_dataframes\n",
    "\n",
    "# Define the base path\n",
    "base_folder = \"SalesLT\"\n",
    "mount_point = \"/mnt/bronze\"  # Make sure this is your correct mount point\n",
    "base_path = f\"{mount_point}/{base_folder}\"\n",
    "\n",
    "# Read files dynamically and store them as named DataFrames\n",
    "created_dataframes = read_files_dynamically(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09105f86-35bd-4026-a2c9-69771414ee37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Purpose**\n",
    "To convert all timestamp type columns in dynamically created DataFrames to date type with the format yyyy-MM-dd.\n",
    "\n",
    "**Process**\n",
    "- Traverse through the schema of each DataFrame.\n",
    "- Collect all columns with the data type timestamp.\n",
    "- Convert identified columns to date type using date_format() function.\n",
    "- Format applied: 'yyyy-MM-dd'.\n",
    "- Replace the original DataFrame variables with the modified DataFrames using globals().\n",
    "- Save them to Silver layer in DELTA format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82257336-aa3e-4b67-9818-3e3197a7ee49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def convert_timestamps_to_date(dataframe: DataFrame) -> DataFrame:\n",
    "    timestamp_columns = [field.name for field in dataframe.schema.fields if field.dataType.typeName() == 'timestamp']\n",
    "    \n",
    "    for column in timestamp_columns:\n",
    "        dataframe = dataframe.withColumn(column, F.date_format(F.col(column), 'yyyy-MM-dd').cast('date'))\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def process_all_dataframes(created_dataframes):\n",
    "    for dataframe_name in created_dataframes:\n",
    "        original_df = globals()[dataframe_name]\n",
    "        \n",
    "        # Convert timestamp columns to date format\n",
    "        modified_df = convert_timestamps_to_date(original_df)\n",
    "        \n",
    "        # Update the global variable with the modified DataFrame\n",
    "        globals()[dataframe_name.lower()] = modified_df\n",
    "        folder_name = dataframe_name.capitalize().split('_')[0]\n",
    "        silver_path = f\"/mnt/silver/{base_folder}/{folder_name}/\"\n",
    "        # Save DataFrame as Delta format to 'silver' container in overwrite mode\n",
    "        modified_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n",
    "# Process all created DataFrames\n",
    "process_all_dataframes(created_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3852b4ba-2103-4144-8805-6744555263e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze-to-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}